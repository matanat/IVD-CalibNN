{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import optuna\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "from regression_net import RegressionNet\n",
    "from data import denormalize_columns, normalize_columns, FEATURES\n",
    "from optimization import *\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# black magic for reproducibility \n",
    "random_state = 42\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with saved parameters.\n"
     ]
    }
   ],
   "source": [
    "# Load the model and parameters from the file\n",
    "saved_model_params = torch.load('trained_models/model_large_rom.pth')\n",
    "\n",
    "# Reconstruct the model\n",
    "loaded_model = RegressionNet(saved_model_params['hparams'])\n",
    "loaded_model.load_state_dict(saved_model_params['state_dict'])\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "\n",
    "feature_names = saved_model_params['feature_names']\n",
    "feature_bounds = saved_model_params['feature_bounds']\n",
    "\n",
    "target_features = ['y_ROM']\n",
    "load_cases = ['Flexion', 'AxialRotation', 'Extension', 'LateralBending']\n",
    "params_columns = ['C10Nucleus', 'C01Nucleus', 'C10Annulus', 'K1Annulus', 'K2Annulus', 'Kappa', 'K1Circ', 'K2Circ', 'K1Rad', 'K2Rad', 'FiberAngle', 'FiberAngleCirc', 'FiberAngleRad']\n",
    "\n",
    "print(\"Model loaded with saved parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = pd.read_csv(\"datasets/Large/trainval_df_large_rom_norm.csv\"), pd.read_csv(\"datasets/Test/test_df_large_rom_norm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or try with a real sample from Heuer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the CSV data\n",
    "df_real_instance = pd.read_csv(\"samples/ROMData_ExperimentsHeuer.csv\", sep=\";\")\n",
    "\n",
    "# Parse the CSV\n",
    "df_real_instance = df_real_instance.melt(id_vars='Moment', var_name='LoadCase', value_name='y_ROM')\n",
    "df_real_instance.rename({'Moment': 'LoadCase', 'LoadCase': 'Moment'}, axis=1, inplace=True)\n",
    "df_real_instance['LoadCase'] = df_real_instance['LoadCase'].apply(lambda x: load_cases.index(x))\n",
    "df_real_instance['Moment'] = df_real_instance['Moment'].astype('float')\n",
    "df_real_instance = df_real_instance[df_real_instance['Moment'] != 0]\n",
    "df_real_instance['y_IDP'] = 0\n",
    "\n",
    "df_real_instance = normalize_columns(df_real_instance, feature_bounds, train=False, keep_target=True)\n",
    "df_real_instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or real sample from experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the CSV data\n",
    "df_exp = pd.read_csv(\"datasets/ValidationCalibration/Nicolini_L1L2/ExperimentalResultsIVDNic3.txt\", sep=\"\\t\")\n",
    "df_exp.drop([0,1], axis=0, inplace=True)\n",
    "df_exp.rename({'Unnamed: 0': 'Moment'}, axis=1, inplace=True)\n",
    "df_exp.columns = [col.replace(\"(degrees)\", \"\").replace(\" \", \"\") if col.endswith(\" (degrees)\") else col for col in df_exp.columns]\n",
    "df_exp = df_exp.melt(id_vars='Moment', var_name='LoadCase', value_name='y_ROM')\n",
    "df_exp['LoadCase'] = df_exp['LoadCase'].apply(lambda x: load_cases.index(x))\n",
    "df_exp = df_exp.astype(float)\n",
    "\n",
    "df_exp = normalize_columns(df_exp, feature_bounds, train=False, keep_target=True)\n",
    "df_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random configuration and distance: 700 0.12709513105247444\n"
     ]
    }
   ],
   "source": [
    "# Define target and init sample\n",
    "closest_config = get_rand_train_sample(df_exp, df_train, verbose=True)\n",
    "\n",
    "df_exp = df_exp.sort_values(['LoadCase', 'Moment'], ignore_index=True)\n",
    "\n",
    "target = df_exp\n",
    "\n",
    "# Use all rows from the target, set with start config\n",
    "init_sample = target.copy()\n",
    "for col in params_columns:\n",
    "    init_sample[col] = closest_config.loc[col]\n",
    "init_sample = init_sample[FEATURES + ['y_ROM']]\n",
    "\n",
    "# sort closest_config by the values of the columns LoadCase and Moment\n",
    "init_sample = init_sample.sort_values(['LoadCase', 'Moment'], ignore_index=True)\n",
    "diff_mask = [df_exp['Moment'] == moment for moment in df_exp['Moment'].unique()]\n",
    "\n",
    "init_sample.drop(['y_ROM'], axis=1, inplace=True)\n",
    "target = target[['y_ROM']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert relevant columns to tensor\n",
    "search_tensor = torch.tensor(init_sample.values, dtype=torch.float, requires_grad=True, device=device)\n",
    "target_tensor = torch.tensor(target.values, dtype=torch.float, device=device)\n",
    "\n",
    "# Hyperparams\n",
    "lr = 0.09715010540129915\n",
    "num_steps = 2250\n",
    "lambda_penalty = 1.0142772931593078\n",
    "\n",
    "pos_bound = [FEATURES.index(col) for col in ['K2Annulus', 'K1Annulus', 'Kappa', 'K1Circ', 'K2Circ', 'K1Rad', 'K2Rad']]\n",
    "neg_bound = [FEATURES.index(col) for col in ['C10Nucleus', 'C01Nucleus', 'C10Annulus', 'K2Annulus', 'K1Annulus', 'Kappa', 'K1Circ', 'K2Circ', 'K1Rad', 'K2Rad', 'FiberAngle', 'FiberAngleCirc', 'FiberAngleRad']]\n",
    "\n",
    "original_features = search_tensor.detach().clone()\n",
    "\n",
    "# Define an optimizer for the input sample\n",
    "optimizer = torch.optim.Adam([search_tensor], lr=lr)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass: compute the predicted outputs\n",
    "    predictions = loaded_model(search_tensor)\n",
    "    \n",
    "    # Compute the loss between the predicted and target values, consider only ROM\n",
    "    main_loss = torch.nn.functional.l1_loss(predictions, target_tensor)\n",
    "\n",
    "    # Compute the penalty for out-of-range values (only for updated features)\n",
    "    penalty = out_of_range_loss(search_tensor[:, neg_bound], bound='negative',) + \\\n",
    "        out_of_range_loss(search_tensor[:, pos_bound], bound='positive',)\n",
    "\n",
    "\n",
    "    # Combine the losses\n",
    "    total_loss = main_loss #+ lambda_penalty * penalty\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to the inputs\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # Zero out the gradients for LoadCase and Moment\n",
    "    search_tensor.grad[:, 0:2] = 0\n",
    "\n",
    "    # Update the inputs based on the gradients. Use mean acorss all Cases and Moments\n",
    "    search_tensor.grad = search_tensor.grad.mean(dim=0, keepdim=True).expand_as(search_tensor)\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # projection step\n",
    "        search_tensor[:, neg_bound] = torch.clamp(search_tensor[:, neg_bound], min=0)\n",
    "        search_tensor[:, pos_bound] = torch.clamp(search_tensor[:, pos_bound], max=1)\n",
    "        \n",
    "        predictions = loaded_model(search_tensor)\n",
    "\n",
    "    # Check for improvement\n",
    "    mae = mean_absolute_error(target_tensor.detach().cpu().numpy(), predictions.detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Reults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tensor = search_tensor.detach().clone()\n",
    "\n",
    "lc_masks = [lc == init_sample.values[:,0] for lc in np.unique(init_sample.values[:,0])]\n",
    "\n",
    "# get the final predictions\n",
    "denom_found = denormalize_columns(pd.DataFrame(best_tensor.cpu(), columns=FEATURES), feature_bounds, also_target=True)\n",
    "\n",
    "found_config = denom_found[params_columns].iloc[0]\n",
    "print(\"Found configuration:\")\n",
    "print(found_config)\n",
    "\n",
    "# Evaluate the results per LoadCase\n",
    "with torch.no_grad():\n",
    "    y_true = target.values\n",
    "    y_pred = loaded_model(best_tensor).detach().cpu().numpy()\n",
    "\n",
    "y_true = y_true * (feature_bounds['y_ROM']['max'] - feature_bounds['y_ROM']['min']) + feature_bounds['y_ROM']['min']\n",
    "y_pred = y_pred * (feature_bounds['y_ROM']['max'] - feature_bounds['y_ROM']['min']) + feature_bounds['y_ROM']['min']\n",
    "\n",
    "print(\"Scores: [R2, MAE]\")\n",
    "\n",
    "r2_scores = [r2_score(y_true[mask], y_pred[mask]) for mask in lc_masks]\n",
    "mae_scores = [mean_absolute_error(y_true[mask], y_pred[mask]) for mask in lc_masks]\n",
    "for i in range(len(lc_masks)):\n",
    "    print(f\"    {load_cases[i]}: {r2_scores[i]:.4f}, {mae_scores[i]:.4f}\")\n",
    "print(f\"Mean: {np.mean(r2_scores):.4f}, {np.mean(mae_scores):.4f}\")\n",
    "\n",
    "neg_bounds = out_of_range_loss(search_tensor[:, neg_bound], bound='negative')\n",
    "pos_bounds = out_of_range_loss(search_tensor[:, pos_bound], bound='positive')\n",
    "print(f\"Negative bounds: {neg_bounds.item():.4f}, Positive bounds: {pos_bounds.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the CSV data\n",
    "df_exp = pd.read_csv(\"datasets/ValidationCalibration/Nicolini_L1L2/ExperimentalResultsIVDNic1.txt\", sep=\"\\t\")\n",
    "df_exp.drop([0,1], axis=0, inplace=True)\n",
    "df_exp.rename({'Unnamed: 0': 'Moment'}, axis=1, inplace=True)\n",
    "df_exp.columns = [col.replace(\"(degrees)\", \"\").replace(\" \", \"\") if col.endswith(\" (degrees)\") else col for col in df_exp.columns]\n",
    "df_exp = df_exp.melt(id_vars='Moment', var_name='LoadCase', value_name='y_ROM')\n",
    "df_exp['LoadCase'] = df_exp['LoadCase'].apply(lambda x: load_cases.index(x))\n",
    "df_exp = df_exp.astype(float)\n",
    "\n",
    "# decide if to take all moments or only trained ones\n",
    "#df_exp = df_exp[df_exp['Moment'] <= 5]\n",
    "\n",
    "df_exp = normalize_columns(df_exp, feature_bounds, train=False, keep_target=True)\n",
    "df_exp\n",
    "\n",
    "# Define target and init sample\n",
    "closest_config = get_closest_train_sample(df_exp, df_train, verbose=True)\n",
    "#closest_config = get_rand_train_sample(df_exp, df_train, verbose=True)\n",
    "#closest_config = get_rand_features(params_columns)\n",
    "\n",
    "df_exp = df_exp.sort_values(['LoadCase', 'Moment'], ignore_index=True)\n",
    "\n",
    "target = df_exp\n",
    "\n",
    "# Use all rows from the target, set with start config\n",
    "init_sample = target.copy()\n",
    "for col in params_columns:\n",
    "    init_sample[col] = closest_config.loc[col]\n",
    "init_sample = init_sample[FEATURES + ['y_ROM']]\n",
    "\n",
    "# sort closest_config by the values of the columns LoadCase and Moment\n",
    "init_sample = init_sample.sort_values(['LoadCase', 'Moment'], ignore_index=True)\n",
    "diff_mask = [df_exp['Moment'] == moment for moment in df_exp['Moment'].unique()]\n",
    "\n",
    "init_sample.drop(['y_ROM'], axis=1, inplace=True)\n",
    "target = target[['y_ROM']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LBGFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert relevant columns to tensor, LBFGS need it to be contiguous\n",
    "search_tensor = torch.from_numpy(init_sample.values).contiguous().float()\n",
    "search_tensor.requires_grad = True\n",
    "\n",
    "original_features = search_tensor.detach().clone()\n",
    "\n",
    "target_tensor = torch.tensor(target.values, dtype=torch.float)\n",
    "\n",
    "# Hyperparams\n",
    "num_steps =  190\n",
    "max_iter =  40\n",
    "history_size =  30\n",
    "lambda_penalty =  0.001516139863958438\n",
    "\n",
    "# Define an optimizer for the input sample using LBFGS\n",
    "optimizer = torch.optim.LBFGS([search_tensor], lr=lr, max_iter=max_iter, history_size=history_size)\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    predictions = loaded_model(search_tensor)\n",
    "    main_loss = torch.nn.functional.l1_loss(predictions[:, 0], target_tensor[:, 0])\n",
    "    penalty = out_of_range_loss(search_tensor[:, 2:])\n",
    "    loss = main_loss + lambda_penalty * penalty\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # Store the original state of search_tensor before the update\n",
    "    prev_search_tensor = search_tensor.clone().detach()\n",
    "\n",
    "    # Perform the optimization step\n",
    "    optimizer.step(closure)\n",
    "\n",
    "    # Calculate the change induced by the optimization step\n",
    "    change = search_tensor - prev_search_tensor\n",
    "    \n",
    "    # Calculate the mean change across all samples (for all features except the first two)\n",
    "    mean_change = change.mean(axis=0, keepdim=True)\n",
    "    mean_change[:, 0:2] = 0\n",
    "    \n",
    "    # Apply the mean change to the entire tensor data\n",
    "    with torch.no_grad():\n",
    "        search_tensor.data = prev_search_tensor + mean_change\n",
    "\n",
    "# Compute metrics\n",
    "with torch.no_grad():\n",
    "    predictions = loaded_model(search_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the final predictions\n",
    "search_result_df = pd.DataFrame(search_tensor.detach().numpy(), columns=FEATURES)\n",
    "denom_found = denormalize_columns(search_result_df, feature_bounds, also_target=True)\n",
    "\n",
    "found_config = denom_found[params_columns].iloc[0]\n",
    "print(\"Found configuration:\")\n",
    "print(found_config)\n",
    "\n",
    "# evaluate the results, also per LoadCase\n",
    "mae = mean_absolute_error(target_tensor.numpy()[:, 0], predictions.detach().numpy()[:, 0])\n",
    "r2 = r2_score(target_tensor.numpy()[:, 0], predictions.detach().numpy()[:, 0])\n",
    "print(f\"Mean MAE: {mae:.4f}, R2: {r2:.4f}\")\n",
    "\n",
    "\n",
    "for i in range(len(load_cases)):\n",
    "    mask = (denom_found.LoadCase.values == i)\n",
    "    mae = mean_absolute_error(target_tensor.numpy()[mask], predictions.detach().numpy()[mask])\n",
    "    r2 = r2_score(target_tensor.numpy()[mask], predictions.detach().numpy()[mask])\n",
    "    print(f\"{load_cases[i]}, MAE: {mae:.4f}, R2: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
